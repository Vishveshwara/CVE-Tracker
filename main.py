import sys
from pypdf import PdfReader
import re
import pandas as pd
import concurrent.futures
import itertools
from bs4 import BeautifulSoup
import requests
import warnings
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
import os
import socket


def rh_scrape(cve):
    warnings.filterwarnings("ignore", category=DeprecationWarning)
    url = 'https://access.redhat.com/security/cve/'
    url = url + cve

    options = webdriver.ChromeOptions()
    options.add_argument('--headless')  # Run Chrome in headless mode
    driver = webdriver.Chrome(options=options)

    try:
        driver.get(url)
        driver.execute_script("window.scrollTo(0,document.body.scrollHeight)")

        loop = 1
        html = driver.page_source

        soup = BeautifulSoup(html, "html.parser")
        paragraph = soup.find('div', class_='pfe-m-7-col-on-md').find_all('p')
        paragraph_text = paragraph[1].get_text(strip=True)

        print("CVE-ID:", cve)
        print(paragraph_text)

        tables = pd.read_html(html)
        f_table = tables[0]

        while loop:
            elm = driver.find_elements(By.ID, 'DataTables_Table_0_next')
            wait = WebDriverWait(driver, 1)

            try:
                wait.until(EC.visibility_of(elm[0]))
            except:
                break

            driver.execute_script("arguments[0].click();", elm[0])
            html = driver.page_source
            tables = pd.read_html(html)
            f_table = pd.concat([f_table, tables[0]], ignore_index=True)
            elm = driver.find_elements(By.ID, 'DataTables_Table_0_next')
            flag = elm[0].get_attribute('tabindex')

            if flag == '-1':
                loop = 0
                break

        # EXCEL INSERTION
        result_data = {
            'CVE-ID': [cve],
            'Description': [paragraph_text]
        }

        # Create a DataFrame using the initial data
        result_df = pd.DataFrame(result_data)
        table_df = pd.DataFrame(f_table)

        # Append the table DataFrame as new rows
        result_df = pd.concat([result_df, table_df], ignore_index=True)

        # Save the DataFrame to an Excel file
        output_folder = "redhat_cve"
        output_filename = f"{output_folder}/{cve}.xlsx"
        result_df.to_excel(output_filename, index=False, header=True)  # Start writing from the second row
        print(f"Data saved to {output_filename}")

        return f_table
    except Exception as e:
        print(f"Website for CVE {cve} not found. Error: {str(e)}")
        # Return a table with NA values
        return pd.DataFrame({
            'CVE-ID': ['NA'],
            'Description': ['NA'],
            'Platform': ['NA'],
            'Package': ['NA'],
            'State': ['NA'],
            'Errata': ['NA'],
            'Release Date': ['NA']
        })
    finally:
        driver.close()


def rh_alma_scrape(cve):
    warnings.filterwarnings("ignore", category=DeprecationWarning)
    url = 'https://access.redhat.com/security/cve/'
    url = url + cve

    options = webdriver.ChromeOptions()
    options.add_argument('--headless')  # Run Chrome in headless mode
    driver = webdriver.Chrome(options=options)

    try:
        driver.get(url)
        driver.execute_script("window.scrollTo(0,document.body.scrollHeight)")

        loop = 1
        html = driver.page_source

        soup = BeautifulSoup(html, "html.parser")
        paragraph = soup.find('div', class_='pfe-m-7-col-on-md').find_all('p')
        paragraph_text = paragraph[1].get_text(strip=True)

        print("CVE-ID:", cve)
        print(paragraph_text)

        tables = pd.read_html(html)
        f_table = tables[0]

        while loop:
            elm = driver.find_elements(By.ID, 'DataTables_Table_0_next')
            wait = WebDriverWait(driver, 1)

            try:
                wait.until(EC.visibility_of(elm[0]))
            except:
                break

            driver.execute_script("arguments[0].click();", elm[0])
            html = driver.page_source
            tables = pd.read_html(html)
            f_table = pd.concat([f_table, tables[0]], ignore_index=True)
            elm = driver.find_elements(By.ID, 'DataTables_Table_0_next')
            flag = elm[0].get_attribute('tabindex')

            if flag == '-1':
                loop = 0
                break

        print(f_table)
    except Exception as e:
        print(f"Website for CVE {cve} not found. Error: {str(e)}")
        # Return a table with NA values
        return pd.DataFrame({
            'CVE-ID': ['NA'],
            'Description': ['NA'],
            'Platform': ['NA'],
            'Package': ['NA'],
            'State': ['NA'],
            'Errata': ['NA'],
            'Release Date': ['NA']
        })
    finally:
        driver.close()



def ubu_scrape(cve):
    url = f"http://ubuntu.com/security/{cve}"
    warnings.filterwarnings("ignore", category=DeprecationWarning)

    response = requests.get(url)
    if response.status_code != 200:
        raise Exception(f"Failed to fetch {url}")

    html = response.text
    soup = BeautifulSoup(html, "html.parser")

    table_data = []
    paragraph = soup.find('div', class_='col-9').find_all('p')
    paragraph_text = paragraph[1].get_text(strip=True)

    for row in soup.find_all('table', class_='cve-table')[0].find_all('tr'):
        row_data = [cell.get_text(strip=True) for cell in row.find_all('td')]
        if row_data:
          if len(row_data) == 3:
            z = row_data[0]
          if len(row_data) < 3:
            a = row_data[0]
            b = row_data[1]
            row_data = [z,a,b]
          table_data.append(row_data)

    if not table_data:
        raise Exception("No table data found")

    data = table_data[:]
    table = pd.DataFrame(data, columns=['Package', 'Release', 'Status'])
    table['Status'] = table['Status'].apply(lambda x: str(x).replace('\n', '').strip())
    table['Status'] = table['Status'].apply(lambda x: re.sub(r'\s+', ' ', x))
    '''print("Row 1")
    print(table.iloc[0])
    print("Row 2")
    print(table.iloc[1])'''

    if 'Release' in table.columns:
        table["Release"].replace({
            'trusty': '14.04 LTS (Trusty Tahr)',
            'xenial': '16.04 LTS (Xenial Xerus)',
            'bionic': '18.04 LTS (Bionic Beaver)',
            'focal': '20.04 LTS (Focal Fossa)',
            'groovy': '20.10 (Groovy Gorilla)',
            'hirsute': '21.04 (Hirsute Hippo)',
            'impish': '21.10 (Impish Indri)',
            'jammy': '22.04 LTS (Jammy Jellyfish)'
        }, inplace=True)

    print("CVE-ID",cve)
    print(paragraph_text)
    #print(table)

    result_data = {
        'CVE-ID': [cve],
        'Description': [paragraph_text]
    }

    # Create a DataFrame using the initial data
    result_df = pd.DataFrame(result_data)
    table_df = pd.DataFrame(table)

    # Append the table DataFrame as new rows
    result_df = pd.concat([result_df, table_df], ignore_index=True)

    # Save the DataFrame to an Excel file
    output_folder = "ubuntu_cve"
    output_filename = f"{output_folder}/{cve}.xlsx"
    result_df.to_excel(output_filename, index=False, header=True)  # Start writing from the second row
    print(f"Data saved to {output_filename}")

    return table

def read_excel_ubu(cve):
    excel_file_path = f"ubuntu_cve/{cve}.xlsx"
    df = pd.read_excel(excel_file_path)
    print("CVE-ID",cve)
    print(df['Description'].iloc[0])
    df = df[['Package', 'Release', 'Status']]
    df = df.drop(index=0)
    #print(df.to_string(index=False))
    return df

def read_excel_rh(cve):
    excel_file_path = f"redhat_cve/{cve}.xlsx"
    df = pd.read_excel(excel_file_path)
    print("CVE-ID", cve)
    print(df['Description'].iloc[0])
    df = df[['Platform', 'Package', 'State', 'Errata', 'Release Date']]
    df = df.drop(index=0)
    #print(df.to_string(index=False))
    return df

def read_excel_rocky(cve):
    excel_file_path = f"rocky_cve/{cve}.xlsx"
    df = pd.read_excel(excel_file_path)
    print("CVE-ID",cve)
    print('Description:')
    print(df['Description'].iloc[0].strip())
    df = df[['Severity', 'CVSS', 'Published', 'Created', 'Added', 'Modified', 'Solutions']]
    df = df.drop(index=0)
    x = df['Solutions'].iloc[0]
    list_of_strings = list(x)
    list_of_strings = x.split(',')
    print(df.to_string(index=False))
    print('Solutions:')
    for i in list_of_strings:
        print(i)

def read_excel_alma(id):
    excel_file_path = f"alma_cve/{id}.xlsx"
    df = pd.read_excel(excel_file_path)
    print("CVE-ID",id)
    print('Description:')
    print(df['Description'].iloc[0].strip())
    df = df[['CVE_ID', 'Packages']]
    df = df.drop(index=0)
    print(df.to_string(index=False))

def alma_scrape(errata_id,version):
    # Create the URL using the input errata ID
    url = f"https://errata.almalinux.org/{version}/{errata_id}.html"

    # Send a GET request to the URL
    response = requests.get(url)
    html_content = response.content
    soup = BeautifulSoup(html_content, "html.parser")

    medium_9_div = soup.find_all('div', class_='medium-9 columns')
    print(len(medium_9_div))
    if len(medium_9_div) >= 5:
        # Get the fourth div (index 3) since it starts from 0
        me = medium_9_div[3]

        ul_with_style = me.find('ul', style='list-style: none; margin: 0;')

        if ul_with_style:
            # Extracting CVE IDs from li elements within the ul
            cve_links = ul_with_style.find_all('a')
            cve_ids = [link.get_text() for link in cve_links]

            # Create a new list containing only non-empty items that start with "CVE"
            filtered_cve_ids = [cve for cve in cve_ids if cve.startswith("CVE")]

            description = soup.find('div', class_='description').get_text(strip=True)

            package_list_items = soup.select('ul.package_list li')
            updated_packages = [item.get_text(strip=True) for item in package_list_items]

            print("Description:")
            print(description)

            print("\nFiltered CVE IDs:")
            for cve_id in filtered_cve_ids:
                print(cve_id)

            print("\nUpdated Packages:")
            for package in updated_packages:
                print(package)

            result_data = {
                'ERRATA-ID': [errata_id],
                'Description': [description]
            }
            result_df = pd.DataFrame(result_data)
            max_length = max(len(filtered_cve_ids), len(updated_packages))
            filtered_cve_ids += [''] * (max_length - len(filtered_cve_ids))
            updated_packages += [''] * (max_length - len(updated_packages))

            data_list = []
            for cve_id, package in zip(filtered_cve_ids, updated_packages):
                data_list.append({
                    'CVE_ID': cve_id,
                    'Packages': package
                })

            # Create a DataFrame from the list of dictionaries
            table_df = pd.DataFrame(data_list)

            result_df = pd.concat([result_df, table_df], ignore_index=True)
            #print(result_df)
            # Save the concatenated DataFrame to an Excel file
            output_folder = "alma_cve"
            output_filename = f"{output_folder}/{errata_id}.xlsx"
            result_df.to_excel(output_filename, index=False, header=True)
            print(f"Data saved to {output_filename}")

        else:
            print("No ul element with the specified style found.")
    else:
        print("Not enough divs with class 'medium-9 columns' found.")

def rocky_scrape(cve):
    url = f"https://www.rapid7.com/db/vulnerabilities/rocky_linux-{cve}/"
    # Send a GET request to the URL
    response = requests.get(url)
    html_content = response.content
    soup = BeautifulSoup(html_content, "html.parser")
    table_section = soup.find('section', class_='tableblock')
    table_data = {}
    for div in table_section.find_all('div', class_='table-col'):
        header = div.find('header').get_text()
        value = div.find('div', class_='table-cell').get_text()
        table_data[header] = value


    # Extract the description
    description_section = soup.find('section', class_='vulndb__detail-wrapper')
    description = description_section.find('div', class_='vulndb__detail-content').get_text()
    description = description.strip()
    description = re.sub(r"Description\s*", "", description)

    # Extract the solutions
    solutions_section = soup.find('section', class_='vulndb__references bottom-border')
    solutions = [li.get_text() for li in solutions_section.find_all('li')]
    table_data['Solutions'] = solutions
    table_df = pd.DataFrame([table_data])

    print("\nDescription:")
    print(description)
    print("Table Data:")
    print(table_df)
    print("\nSolutions:")
    for solution in solutions:
        print(solution)


    result_data = {
        'CVE-ID': [cve],
        'Description': [description]
    }

    # Create a DataFrame using the initial data
    result_df = pd.DataFrame(result_data)

    # Append the table data (including solutions) as a new row
    #result_df = result_df.append(table_data, ignore_index=True)

    result_df = pd.concat([result_df, table_df], ignore_index=True)

    # Save the DataFrame to an Excel file
    #print(result_df)
    output_folder = "rocky_cve"
    output_filename = f"{output_folder}/{cve}.xlsx"
    result_df.to_excel(output_filename, index=False, header=True)
    print(f"Data saved to {output_filename}")




def tab_search(cve, version,local):
    if (local==1):
        table = read_excel_ubu(cve)
    else:
        table = ubu_scrape(cve)
    patch = table[table['Release'].str.contains(version, regex=False, na=False)]
    patch.insert(0, 'CVE', cve)

    #print(cve, " ", patch['Status'].values)
    if not patch[['Package','Release','Status']].values.any():
        print("Not found")
        # If no matching rows were found, append a row with "Not Found" information to the DataFrame
        patch = patch.append({
            'CVE': cve,
            'Version': version,
            "Package": "Not Found",
            "Release": "Not Applicable",
            "Status": "Not applicable"
        }, ignore_index=True)
    else:
        print("Found")
    return patch


def tab_searchr(cve, os_name, version,local):
    os_final = os_name + " " + version
    if (local == 1):
        table = read_excel_rh(cve)
    else:
        table = rh_scrape(cve)
    os_name = os_name.rsplit(' ', 1)[0]
    patch = table[table['Platform'].str.contains(os_final, regex=False, na=False)]
    #print("PATCHHH = ",patch)
    pd.set_option('display.max_colwidth', None)
    patch.insert(0, 'CVE', cve)
    print(cve)

    # Check if any rows were found that match the filtering criteria
    if not patch[['State', 'Package']].values.any():
        print("Patch Not found")
        # If no matching rows were found, append a row with "Not Found" information to the DataFrame
        patch = patch.append({
            'CVE': cve,
            'Platform': os_final,
            "Package": "Not Found",
            "State": "Not Applicable",
            "Errata": "Not applicable",
            "Release Date": "Not Applicable"
        }, ignore_index=True)
    else:
        print("Patch Found")
    print(patch)
    return patch


def table(oper_sys_name, cve1, os_name, version,local):
    table = pd.DataFrame()

    if oper_sys_name.startswith('Red Hat'):
        with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:
            patches = list(executor.map(tab_searchr, cve1, itertools.repeat(os_name), itertools.repeat(version),itertools.repeat(local)))
            table = pd.concat(patches, ignore_index=True)


    if oper_sys_name.startswith('Ubuntu'):
        with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:
            patches = list(executor.map(tab_search, cve1, itertools.repeat(version),itertools.repeat(local)))
            table = pd.concat(patches, ignore_index=True)

    return table

def extract_os_and_cve(pdf_path):
    os_info_pattern = r'(Ubuntu|Rocky|Alma|Red Hat Enterprise)\s?Linux( Server)? [\d.]+'
    cve_pattern = r'CVE-\d{4}-\d{4,7}'
    als_pattern = r'[A-Z]+-\d{4}:\d{4,7}'

    pdf = PdfReader(pdf_path)
    os_info = None
    cve_list = []
    als_list = []

    for i in range(len(pdf.pages)):
        page = pdf.pages[i]
        x = page.extract_text()
        lines = x.split('\n')

        for line in lines:
            os_match = re.search(os_info_pattern, line)
            if os_match:
                os_info = os_match.group(0)
            cve_match = re.search(cve_pattern, line)
            if cve_match:
                cve_list.append(cve_match.group(0))
            als_match = re.search(als_pattern, line)
            if als_match:
                als_list.append(als_match.group(0))

    return os_info, cve_list, als_list

def check_internet_connection():
    try:
        # Try connecting to Google's DNS server
        socket.create_connection(("8.8.8.8", 53))
        return 1  # Internet connection is available
    except OSError:
        return 0
def get_cve_excel_path(cve, os_name):

    if "Ubuntu" in os_name:
        excel_path = f"ubuntu_cve/{cve}.xlsx"
    elif "Red Hat" in os_name:
        excel_path = f"redhat_cve/{cve}.xlsx"
    elif "Rocky" in os_name:
        excel_path = f"rocky_cve/{cve}.xlsx"
    elif "Alma" in os_name:
        excel_path = f"alma_cve/{cve}.xlsx"
    # Add more cases for other OS if needed

    if excel_path is None or not os.path.exists(excel_path):
        return 0

    return 1


def patch_available_rk(cvel):
    internet = check_internet_connection()
    for cve in cvel:
        print("CVE-ID: ",cve)
        local = get_cve_excel_path(cve, 'Rocky Linux')
        if local == 1:
            print("Found Locally")
            read_excel_rocky(cve)
        else:
            print("Used Scraping")
            if internet == 1:
                rocky_scrape(cve)
            else:
                print("NO SOLUTION LOCALLY")

def patch_available_al(cvel,version):
    internet = check_internet_connection()
    for cve in cvel:
        print("CVE-ID: ",cve)
        local = get_cve_excel_path(cve, 'Alma Linux')
        if local == 1:
            print("Found Locally")
            read_excel_alma(cve)
        else:
            if internet == 1:
                alma_scrape(cve,version)
            else:
                print("NO SOLUTION LOCALLY")


def patch_available_rh_ubu(oper_sys_name, cve, os_name, version):
    warnings.filterwarnings("ignore", category=FutureWarning)
    result_df = pd.DataFrame()
    internet = check_internet_connection()
    for i in cve:
        local = get_cve_excel_path(i, os_name)
        if local==1:
            df = table(oper_sys_name, [i], os_name, version,local)
            p = 0
            if oper_sys_name.startswith('Red Hat'):
                affected_rows = df[df['State'] == 'Affected']
                affected_cve_values = affected_rows['CVE'].tolist()
                for cve_values in affected_cve_values:
                    if internet == 1:
                        z = table(oper_sys_name, [i], os_name, version, 0)
                        if df.equals(z):
                            print('No Update patch present for ',i)
                            result_df = result_df.append(df, ignore_index=True)
                            p = 1
                        else:
                            print('Update patch present')
                            result_df = result_df.append(z, ignore_index=True)
                            p = 1
                    else:
                        print("Not able to check the update patch for the present date")
                        result_df = result_df.append(df, ignore_index=True)
                        p = 1
            if p == 0:
                result_df = result_df.append(df, ignore_index=True)
        else:
            if internet == 1:
                df = table(oper_sys_name, [i], os_name, version, local)
                result_df = result_df.append(df, ignore_index=True)
            else:
                print("NO SOLUTION LOCALLY")
                if oper_sys_name.startswith('Red Hat'):
                    result_df = result_df.append({
                        'CVE': i,
                        'Platform': os_name + " " +version,
                        "Package": "Not Found",
                        "State": "Not Applicable",
                        "Errata": "Not applicable",
                        "Release Date": "Not Applicable"
                    }, ignore_index=True)
                else:
                    result_df = result_df.append({
                        'CVE': i,
                        'Version': version,
                        "Package": "Not Found",
                        "Release": "Not Applicable",
                        "Status": "Not applicable"
                    }, ignore_index=True)
    warnings.filterwarnings("ignore", category=FutureWarning)
    return result_df

def patch_available_ubu(oper_sys_name, cvel, version):
    warnings.filterwarnings("ignore", category=FutureWarning)
    internet = check_internet_connection()
    result_df = pd.DataFrame()
    for cve in cvel:
        local = get_cve_excel_path(cve, 'Ubuntu')
        if local == 1:
            print("Found Locally")
            df = table(oper_sys_name, [cve], 'Ubuntu', version,local)
        else:
            print("Used Scraping")
            if internet == 1:
                df = table(oper_sys_name, [cve], 'Ubuntu', version,local)
            else:
                print("NO SOLUTION LOCALLY")
                df = pd.DataFrame({
                    'CVE': cve,
                    'Version': version,
                    "Package": "Not Found",
                    "Release": "Not Applicable",
                    "Status": "Not applicable"
                })
        result_df = result_df.append(df, ignore_index=True)
    return result_df


if __name__ == '__main__':
    pd.set_option('expand_frame_repr', False)
    pd.set_option('display.max_rows',2000)
    pd.set_option('display.max_columns', 2000)
    pd.set_option('display.width', 2000)

    OS, extracted_cve,als = extract_os_and_cve('C:/Users/Vishv/Downloads/209.pdf') #ENTER THE PATH IN THIS FORMAT

    print("Operating System")
    print(OS)

    try:
        extracted_cve.remove('CVE-2020-14145')
    except ValueError:
        print("CVE-2020-14145 does not exist in the list.")

    print("CVE IDs")
    for pattern in list(extracted_cve):
        print(pattern)

    new_set_of_strings = set()
    for string in als:
        new_string = string.replace(":", "-")
        new_set_of_strings.add(new_string)


    if OS.startswith('Ubuntu'):
        oper_sys_name = "Ubuntu"
        os_name = "Ubuntu Linux"
        version = re.search(r"(\d+\.\d+)", OS).group(0)
        df = patch_available_ubu(oper_sys_name, extracted_cve, version)
        print("--------FINAL DATAFRAME ----------- ")
        print(df)
        sys.exit()

    if OS.startswith('Alma'):
        if len(new_set_of_strings)==0:
            for i in extracted_cve:
                rh_alma_scrape(i)
        else:
            oper_sys_name = "Alma"
            version = OS.split(".")[0][-1]
            patch_available_al(list(new_set_of_strings),version)
        sys.exit()

    if OS.startswith('Rocky'):
        patch_available_rk(extracted_cve)
        sys.exit()

    if OS.startswith('Red'):
        version = OS.split(".")[0][-1]
        os_name = ' '.join(OS.split()[:-1])
        if 'Server' in os_name:
            os_name = os_name.replace('Server', "").strip()
        df = patch_available_rh_ubu(OS,extracted_cve,os_name,version)
        print("--------FINAL DATAFRAME ----------- ")
        print(df)
        sys.exit()

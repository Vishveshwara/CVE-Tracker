import subprocess
import sys
from pypdf import PdfReader
import re
import pandas as pd
import concurrent.futures
import itertools
from bs4 import BeautifulSoup
import requests
import warnings
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from flask import Flask, render_template, redirect, url_for
from flask_wtf import FlaskForm
from wtforms import FileField, SubmitField
from werkzeug.utils import secure_filename
import os
from wtforms.validators import InputRequired
import importlib.util


app = Flask(__name__)
app.config['SECRET_KEY'] = 'supersecretkey'
app.config['UPLOAD_FOLDER'] = 'static/files'
def extract_os_and_cve(pdf_path):
    os_info_pattern = r'(Ubuntu|Rocky|Alma|Red Hat Enterprise)\s?Linux( Server)? [\d.]+'
    cve_pattern = r'CVE-\d{4}-\d{4,7}'
    rhsa_pattern = r'RHSA-\d{4}:\d{4,7}'
    rlsa_pattern = r'RLSA-\d{4}:\d{4,7}'
    alsa_pattern = r'ALSA-\d{4}:\d{4,7}'

    pdf = PdfReader(pdf_path)
    os_info = None
    cve_set = set()
    rhsa_set = set()
    rlsa_set = set()
    alsa_set = set()

    for i in range(len(pdf.pages)):
        page = pdf.pages[i]
        x = page.extract_text()
        lines = x.split('\n')

        for line in lines:
            os_match = re.search(os_info_pattern, line)
            if os_match:
                os_info = os_match.group(0)
            cve_match = re.search(cve_pattern, line)
            if cve_match:
                cve_set.add(cve_match.group(0))
            rhsa_match = re.search(rhsa_pattern, line)
            if rhsa_match:
                rhsa_set.add(rhsa_match.group(0))
            rlsa_match = re.search(rlsa_pattern, line)
            if rlsa_match:
                rlsa_set.add(rlsa_match.group(0))
            alsa_match = re.search(alsa_pattern, line)
            if alsa_match:
                alsa_id = alsa_match.group(0)
                # Modify ALSA pattern to store data like ALSA-2022-7519
                alsa_id = alsa_id.replace(':', '-')
                alsa_set.add(alsa_id)

    # Convert sets back to lists before returning
    return os_info, list(cve_set), list(rhsa_set), list(rlsa_set), list(alsa_set)


def GetSolution(pdf_path):
    pdf = PdfReader(pdf_path)

    # Define the patterns to find SOLUTION and RESULTS sections
    solution_pattern = r"SOLUTION:(.*?)RES(.*?)"

    solutions = []

    for page in pdf.pages:
        text = page.extract_text()
        solution_matches = re.findall(solution_pattern, text, re.DOTALL)

        for solution_match in solution_matches:
            solution_text = solution_match[0].strip()
            solutions.append(solution_text)

    return solutions


def search_google_for_rlsas(cves):
    rlsa_set = set()
    rlsa_pattern = r'RLSA-\d{4}[:\-]\d{4,7}'

    for cve in cves:
        query = f"{cve} rocky linux"
        url = f"https://www.google.com/search?q={query}"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

        response = requests.get(url, headers=headers)

        if response.status_code == 200:
            soup = BeautifulSoup(response.content, "html.parser")
            html_content = str(soup)

            if "RLSA" in html_content:
                rlsa_matches = re.findall(rlsa_pattern, html_content)
                standardized_rlsa = [rlsa.replace('-', ':') for rlsa in rlsa_matches]
                rlsa_set.update(standardized_rlsa)

        else:
            print(f"Failed to retrieve search results for {cve}")

    return rlsa_set

def get_ubuntu_releases():
    url = "https://releases.ubuntu.com/"
    response = requests.get(url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, "html.parser")
        releases = {}

        # Extracting LTS Releases
        lts_releases = soup.find_all('h4', text='LTS Releases')[0].find_next('ul', class_='p-list')
        for release in lts_releases.find_all('a'):
            release_version = release.get_text(strip=True).split(' ')[1]
            release_name = release.get_text(strip=True).split('(')[0].strip()
            release_code_name = release.get_text(strip=True).split('(')[1].split()[0]
            releases[release_code_name.lower()] = f"{release_version} LTS ({release_code_name})"

        # Extracting Extended Security Maintenance (ESM) Releases
        esm_releases = soup.find_all('h3', text='Extended Security Maintenance (ESM)')[0].find_next('ul', class_='p-list')
        for release in esm_releases.find_all('a'):
            release_version = release.get_text(strip=True).split(' ')[1]
            release_name = release.get_text(strip=True).split('(')[0].strip()
            release_code_name = release.get_text(strip=True).split('(')[1].split()[0]
            releases[release_code_name.lower()] = f"{release_version} LTS ({release_code_name})"

        return releases
    else:
        return None
def ubu_scrape(cve,dic):
    print(cve)
    try:
        url = f"http://ubuntu.com/security/{cve}"
        warnings.filterwarnings("ignore", category=DeprecationWarning)

        response = requests.get(url)
        if response.status_code != 200:
            raise Exception(f"Failed to fetch {url}")

        html = response.text
        soup = BeautifulSoup(html, "html.parser")

        table_data = []
        paragraph = soup.find('div', class_='col-9').find_all('p')
        try:
            paragraph_text = paragraph[1].get_text(strip=True)
        except:
            paragraph_text = " "

        for row in soup.find_all('table', class_='cve-table')[0].find_all('tr'):
            row_data = [cell.get_text(strip=True) for cell in row.find_all('td')]
            if row_data:
              if len(row_data) == 3:
                z = row_data[0]
              if len(row_data) < 3:
                a = row_data[0]
                b = row_data[1]
                row_data = [z,a,b]
              table_data.append(row_data)

        if not table_data:
            raise Exception("No table data found")


        data = table_data[:]
        table = pd.DataFrame(data, columns=['Package', 'Release', 'Status'])
        table['Status'] = table['Status'].apply(lambda x: str(x).replace('\n', '').strip())
        table['Status'] = table['Status'].apply(lambda x: re.sub(r'\s+', ' ', x))
        '''print("Row 1")
        print(table.iloc[0])
        print("Row 2")
        print(table.iloc[1])'''
        if 'Release' in table.columns:
            table["Release"].replace(dic, inplace=True)

        print("CVE-ID",cve)
        print(paragraph_text)
        #print(table)

        return table,paragraph_text
    except Exception as e:
        print(f"Website for CVE {cve} not found. Error: {str(e)}")
        # Return a table with NA values
        s = f"Website for {cve} not found"
        return pd.DataFrame({
            'Package': ['NA'],
            'Release': ['NA'],
            'Status': ['NA'],
            'Version': ['NA']
        }),s

def rh_scrape(cve):
    warnings.filterwarnings("ignore", category=DeprecationWarning)
    url = 'https://access.redhat.com/security/cve/'
    url = url + cve

    options = webdriver.ChromeOptions()
    options.add_argument('--headless')  # Run Chrome in headless mode
    driver = webdriver.Chrome(options=options)

    try:
        driver.get(url)
        driver.execute_script("window.scrollTo(0,document.body.scrollHeight)")

        loop = 1
        html = driver.page_source

        soup = BeautifulSoup(html, "html.parser")
        paragraph = soup.find('div', class_='pfe-m-7-col-on-md').find_all('p')
        paragraph_text = paragraph[1].get_text(strip=True)

        print("CVE-ID:", cve)
        print(paragraph_text)

        tables = pd.read_html(html)
        f_table = tables[0]

        while loop:
            elm = driver.find_elements(By.ID, 'DataTables_Table_0_next')
            wait = WebDriverWait(driver, 1)

            try:
                wait.until(EC.visibility_of(elm[0]))
            except:
                break

            driver.execute_script("arguments[0].click();", elm[0])
            html = driver.page_source
            tables = pd.read_html(html)

            f_table = pd.concat([f_table, tables[0]], ignore_index=True)
            elm = driver.find_elements(By.ID, 'DataTables_Table_0_next')
            flag = elm[0].get_attribute('tabindex')

            if flag == '-1':
                loop = 0
                break

        # EXCEL INSERTION
        result_data = {
            'CVE-ID': [cve],
            'Description': [paragraph_text]
        }

        # Create a DataFrame using the initial data
        result_df = pd.DataFrame(result_data)
        table_df = pd.DataFrame(f_table)

        # Append the table DataFrame as new rows
        result_df = pd.concat([result_df, table_df], ignore_index=True)

        return f_table,paragraph_text
    except Exception as e:
        print(f"Website for CVE {cve} not found. Error: {str(e)}")
        s = f"Website for {cve} not found"
        # Return a table with NA values
        return pd.DataFrame({
            'CVE-ID': ['NA'],
            'Description': ['NA'],
            'Platform': ['NA'],
            'Package': ['NA'],
            'State': ['NA'],
            'Errata': ['NA'],
            'Release Date': ['NA']
        }),s
    finally:
        driver.close()
def tab_search(cve, version,dic):
    table,s = ubu_scrape(cve,dic)
    patch = table[table['Release'].str.contains(version, regex=False, na=False)]
    patch.insert(0, 'CVE', cve)

    #print(cve, " ", patch['Status'].values)
    if not patch[['Package','Release','Status']].values.any():
        print("Not found")
        # If no matching rows were found, append a row with "Not Found" information to the DataFrame
        patch = patch.append({
            'CVE': cve,
            'Version': version,
            "Package": "Not Found",
            "Release": "Not Applicable",
            "Status": "Not applicable"
        }, ignore_index=True)
    else:
        print("Found")
    return patch,s

def tab_searchr(cve, os_name, version):
    os_final = os_name + " " + version
    table,s = rh_scrape(cve)
    os_name = os_name.rsplit(' ', 1)[0]
    patch = table[table['Platform'].str.contains(os_final, regex=False, na=False)]
    pd.set_option('display.max_colwidth', None)
    patch.insert(0, 'CVE', cve)
    print(cve)

    # Check if any rows were found that match the filtering criteria
    if not patch[['State', 'Package']].values.any():
        print("Patch Not found")
        # If no matching rows were found, append a row with "Not Found" information to the DataFrame
        patch = patch.append({
            'CVE': cve,
            'Platform': os_final,
            "Package": "Not Found",
            "State": "Not Applicable",
            "Errata": "Not applicable",
            "Release Date": "Not Applicable"
        }, ignore_index=True)
    else:
        print("Patch Found")
    return patch,s

def search_google_for_alsas(cves,version):
    rlsa_set = set()
    rlsa_pattern = r'ALSA-\d{4}[:\-]\d{4,7}'

    for cve in cves:
        query = f"{cve} alma linux {version} errata"
        url = f"https://www.google.com/search?q={query}"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

        response = requests.get(url, headers=headers)

        if response.status_code == 200:
            soup = BeautifulSoup(response.content, "html.parser")
            html_content = str(soup)

            if "ALSA" in html_content:
                rlsa_matches = re.findall(rlsa_pattern, html_content)
                standardized_rlsa = [rlsa.replace(':', '-') for rlsa in rlsa_matches]
                rlsa_set.update(standardized_rlsa)

        else:
            print(f"Failed to retrieve search results for {cve}")

    return rlsa_set

def alma_scrape(errata_id,version):
    # Create the URL using the input errata ID
    print(errata_id)
    print('========')
    url = f"https://errata.almalinux.org/{version}/{errata_id}.html"

    try:
        # Send a GET request to the URL
        response = requests.get(url)
        html_content = response.content
        soup = BeautifulSoup(html_content, "html.parser")

        medium_9_div = soup.find_all('div', class_='medium-9 columns')
        if len(medium_9_div) >= 5:
            # Get the fourth div (index 3) since it starts from 0
            me = medium_9_div[3]

            ul_with_style = me.find('ul', style='list-style: none; margin: 0;')

            if ul_with_style:
                # Extracting CVE IDs from li elements within the ul
                cve_links = ul_with_style.find_all('a')
                cve_ids = [link.get_text() for link in cve_links]

                # Create a new list containing only non-empty items that start with "CVE"
                filtered_cve_ids = [cve for cve in cve_ids if cve.startswith("CVE")]

                description = soup.find('div', class_='description').get_text(strip=True)

                table = soup.find('table', class_='sort-table')
                print("Description:")
                print(description)

                print("\nFiltered CVE IDs:")
                for cve_id in filtered_cve_ids:
                    print(cve_id)

                print("\nUpdated Packages:")
                data = []
                for row in table.find_all('tr'):
                    columns = row.find_all('td')
                    if columns:  # Skip header row
                        architecture = columns[0].get_text(strip=True)
                        package = columns[1].get_text(strip=True)
                        checksum = columns[2].get_text(strip=True)
                        data.append({
                            'Architecture': architecture,
                            'Package': package,
                            'Checksum': checksum
                        })
                df = pd.DataFrame(data)
                df.drop(index=df.index[0], axis=0, inplace=True)
                return df,description

            else:
                print("No ul element with the specified style found.")
                s = f"Error in fetching data for {errata_id}"
                return pd.DataFrame({
                    'Architecture': ['NA'],
                    'Package': ['NA'],
                    'Checksum': ['NA'],
                }), s
        else:
            print(f"Website for CVE {errata_id} not found or might be for a different OS version.")
            s = f"Error in fetching data for {errata_id}"
            return pd.DataFrame({
                'Architecture': ['NA'],
                'Package': ['NA'],
                'Checksum': ['NA'],
            }), s

    except Exception as e:
        s = f"Website for {errata_id} not found"
        print(f"Website for CVE {errata_id} not found. Error: {str(e)}")
        return pd.DataFrame({
            'Architecture': ['NA'],
            'Package': ['NA'],
            'Checksum': ['NA'],
        }), s


def process_pdf(file_path):
    OS, cve, rhsa, rlsa, alsa = extract_os_and_cve(file_path)
    print("OS Information:", OS)
    print("CVEs:", cve)
    print("RHSA IDs:", rhsa)
    print("RLSA IDs:", rlsa)
    print("ALSA IDs:", alsa)
    print("SOLUTION:")
    s = GetSolution(file_path)
    d = {}
    for idx, solution in enumerate(s, 1):
        print(f"Solution {idx}:")
        d[idx] = solution
        print(solution)
        print("--------------------")

    if OS.startswith('Rocky'):
        s = search_google_for_rlsas(cve)
        for i in rlsa:
            s.add(i)
        s = list(s)
        formatted_strings = [string.replace(":", "-", 1) for string in s]
        print(formatted_strings)
        return d,formatted_strings

    if OS.startswith('Ubuntu'):
        oper_sys_name = "Ubuntu"
        os_name = "Ubuntu Linux"
        dic = get_ubuntu_releases()
        version = re.search(r"(\d+\.\d+)", OS).group(0)
        DF = []
        S = []
        for i in cve:
            df,s = tab_search(i,version,dic)
            DF.append(df)
            S.append(s)
            print(s)
            print(df)
        return d,DF,S

    if OS.startswith('Red'):
        version = OS.split(".")[0][-1]
        os_name = ' '.join(OS.split()[:-1])
        if 'Server' in os_name:
            os_name = os_name.replace('Server', "").strip()
        DF = []
        S = []
        for i in cve:
            df, s = tab_searchr(i,os_name,version)
            DF.append(df)
            S.append(s)
            print(s)
            print(df)
        return d, DF, S

    if OS.startswith('Alma'):
        oper_sys_name = "Alma"
        version = OS.split(".")[0][-1]
        s = search_google_for_alsas(cve, version)
        for i in alsa:
            s.add(i)
        s = list(s)
        P = s
        print(s)
        DF = []
        S = []
        for i in s:
            x,s = alma_scrape(i, version)
            print(x)
            DF.append(x)
            S.append(s)
        return d, DF, S,P


class UploadFileForm(FlaskForm):
    file = FileField("File", validators=[InputRequired()])
    submit = SubmitField("Upload File")

@app.route('/', methods=['GET', 'POST'])
@app.route('/home', methods=['GET', 'POST'])
def home():
    form = UploadFileForm()
    if form.validate_on_submit():
        file = form.file.data
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], secure_filename(file.filename))

        # Call the processing function from your script
        #Call the exctract_os and cve function and based on that get the return value
        OS, cve, rhsa, rlsa, alsa = extract_os_and_cve(file_path)
        cve_length = len(cve)

        if OS.startswith('Alma'):
            d, DF, S,s = process_pdf(file_path)
            print(s)
            alsa_length = len(s)
            print(len(DF))
            print(alsa_length)
            return render_template('Alma_Results.html', OS=OS, cve=cve, rhsa=rhsa, rlsa=rlsa, alsa=alsa, d=d, DF=DF, S=S, als=s,alsa_length=alsa_length)
        if OS.startswith('Red'):
            d, DF, S = process_pdf(file_path)
            return render_template('RedHat_results.html', OS=OS, cve=cve, rhsa=rhsa, rlsa=rlsa, alsa=alsa, d=d, DF=DF, S=S, cve_length=cve_length)
        if OS.startswith('Rocky'):
            d,s = process_pdf(file_path)
            return render_template('result.html', OS=OS, cve=cve, rhsa=rhsa, rlsa=rlsa, alsa=alsa, d=d, formatted_strings=s, cve_length=cve_length)
        if OS.startswith('Ubuntu'):
            d,DF,S = process_pdf(file_path)
            return render_template('ubuntu_results.html',OS=OS, cve=cve, rhsa=rhsa, rlsa=rlsa, alsa=alsa,d=d, DF=DF, S=S, cve_length=cve_length)

    return render_template('index.html', form=form)

if __name__ == '__main__':
    app.run(debug=True)